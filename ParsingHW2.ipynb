{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1KELKfRAqrssQDWVmOgDdhB1BeZ_El9Xz",
      "authorship_tag": "ABX9TyMS/XH7TSA88Awf1NAVsErr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VialliTem42285/super-journey/blob/main/ParsingHW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install conllu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMjLYy0-eoEY",
        "outputId": "7051134d-47bf-484f-dfaa-3234f026860a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting conllu\n",
            "  Downloading conllu-5.0.1-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading conllu-5.0.1-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-5.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 2to3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRVtRpiyfAXH",
        "outputId": "97cc3291-67c8-4551-c748-af9b78812074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting 2to3\n",
            "  Downloading 2to3-1.0-py3-none-any.whl.metadata (225 bytes)\n",
            "Downloading 2to3-1.0-py3-none-any.whl (1.7 kB)\n",
            "Installing collected packages: 2to3\n",
            "Successfully installed 2to3-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "6dUbtxKwfdwq",
        "outputId": "947fd577-5e17-417c-8a8a-5a0ef8849580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4945cabb-dac1-461d-a151-f8e7e2c0b60a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4945cabb-dac1-461d-a151-f8e7e2c0b60a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Honnible.zip to Honnible.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!2to3 -W Honnible.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pG28G0Ptfl2r",
        "outputId": "7abc5da9-952d-493c-e854-624615cfa97d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --write-unchanged-files/-W implies -w.\n",
            "RefactoringTool: Skipping optional fixer: buffer\n",
            "RefactoringTool: Skipping optional fixer: idioms\n",
            "RefactoringTool: Skipping optional fixer: set_literal\n",
            "RefactoringTool: Skipping optional fixer: ws_comma\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/2to3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/cmd_2to3/__main__.py\", line 6, in main\n",
            "    sys.exit(l2to3_main(\"lib2to3.fixes\"))\n",
            "  File \"/usr/lib/python3.10/lib2to3/main.py\", line 263, in main\n",
            "    rt.refactor(args, options.write, options.doctests_only,\n",
            "  File \"/usr/lib/python3.10/lib2to3/refactor.py\", line 690, in refactor\n",
            "    return super(MultiprocessRefactoringTool, self).refactor(\n",
            "  File \"/usr/lib/python3.10/lib2to3/refactor.py\", line 286, in refactor\n",
            "    self.refactor_file(dir_or_file, write, doctests_only)\n",
            "  File \"/usr/lib/python3.10/lib2to3/refactor.py\", line 731, in refactor_file\n",
            "    return super(MultiprocessRefactoringTool, self).refactor_file(\n",
            "  File \"/usr/lib/python3.10/lib2to3/refactor.py\", line 326, in refactor_file\n",
            "    input, encoding = self._read_python_source(filename)\n",
            "  File \"/usr/lib/python3.10/lib2to3/refactor.py\", line 322, in _read_python_source\n",
            "    return f.read(), encoding\n",
            "  File \"/usr/lib/python3.10/codecs.py\", line 322, in decode\n",
            "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
            "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe8 in position 10: invalid continuation byte\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Honnible.zip', 'r') as file:\n",
        "      print(file.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "JeITgBH2ivLz",
        "outputId": "6ead286e-0dac-451f-aef0-e7eba972896c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0xe8 in position 10: invalid continuation byte",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-7b2ee9074124>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Honnible.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe8 in position 10: invalid continuation byte"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "BopBNX5Kje__",
        "outputId": "ce94e213-164d-421b-ee46-ac2da6fb8af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cdee1bcf-8b61-4011-b2fc-3eff559bf830\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cdee1bcf-8b61-4011-b2fc-3eff559bf830\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving gistfile1.py to gistfile1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!2to3 -W gistfile1.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNXt-smyjyzg",
        "outputId": "a3d9d713-1a9d-4cbc-b914-08eee73fca81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: --write-unchanged-files/-W implies -w.\n",
            "RefactoringTool: Skipping optional fixer: buffer\n",
            "RefactoringTool: Skipping optional fixer: idioms\n",
            "RefactoringTool: Skipping optional fixer: set_literal\n",
            "RefactoringTool: Skipping optional fixer: ws_comma\n",
            "RefactoringTool: Refactored gistfile1.py\n",
            "--- gistfile1.py\t(original)\n",
            "+++ gistfile1.py\t(refactored)\n",
            "@@ -135,7 +135,7 @@\n",
            "         costly.add(SHIFT)\n",
            "     # If there are any dependencies between s0 and the buffer, popping\n",
            "     # s0 will lose them.\n",
            "-    if deps_between(stack[-1], range(n0+1, n-1), gold):\n",
            "+    if deps_between(stack[-1], list(range(n0+1, n-1)), gold):\n",
            "         costly.add(LEFT)\n",
            "         costly.add(RIGHT)\n",
            "     return [m for m in MOVES if m not in costly]\n",
            "@@ -273,13 +273,13 @@\n",
            "     def score(self, features):\n",
            "         all_weights = self.weights\n",
            "         scores = dict((clas, 0) for clas in self.classes)\n",
            "-        for feat, value in features.items():\n",
            "+        for feat, value in list(features.items()):\n",
            "             if value == 0:\n",
            "                 continue\n",
            "             if feat not in all_weights:\n",
            "                 continue\n",
            "             weights = all_weights[feat]\n",
            "-            for clas, weight in weights.items():\n",
            "+            for clas, weight in list(weights.items()):\n",
            "                 scores[clas] += value * weight\n",
            "         return scores\n",
            " \n",
            "@@ -299,9 +299,9 @@\n",
            "             upd_feat(guess, f, weights.get(guess, 0.0), -1.0)\n",
            " \n",
            "     def average_weights(self):\n",
            "-        for feat, weights in self.weights.items():\n",
            "+        for feat, weights in list(self.weights.items()):\n",
            "             new_feat_weights = {}\n",
            "-            for clas, weight in weights.items():\n",
            "+            for clas, weight in list(weights.items()):\n",
            "                 param = (feat, clas)\n",
            "                 total = self._totals[param]\n",
            "                 total += (self.i - self._tstamps[param]) * weight\n",
            "@@ -311,7 +311,7 @@\n",
            "             self.weights[feat] = new_feat_weights\n",
            " \n",
            "     def save(self, path):\n",
            "-        print \"Saving model to %s\" % path\n",
            "+        print(\"Saving model to %s\" % path)\n",
            "         pickle.dump(self.weights, open(path, 'w'))\n",
            " \n",
            "     def load(self, path):\n",
            "@@ -424,8 +424,8 @@\n",
            "                 self.classes.add(tag)\n",
            "         freq_thresh = 20\n",
            "         ambiguity_thresh = 0.97\n",
            "-        for word, tag_freqs in counts.items():\n",
            "-            tag, mode = max(tag_freqs.items(), key=lambda item: item[1])\n",
            "+        for word, tag_freqs in list(counts.items()):\n",
            "+            tag, mode = max(list(tag_freqs.items()), key=lambda item: item[1])\n",
            "             n = sum(tag_freqs.values())\n",
            "             # Don't add rare words to the tag dictionary\n",
            "             # Only add quite unambiguous words\n",
            "@@ -446,10 +446,10 @@\n",
            "             if itn < 5:\n",
            "                 parser.tagger.train_one(words, gold_tags)\n",
            "             total += len(words)\n",
            "-        print itn, '%.3f' % (float(corr) / float(total))\n",
            "+        print(itn, '%.3f' % (float(corr) / float(total)))\n",
            "         if itn == 4:\n",
            "             parser.tagger.model.average_weights()\n",
            "-    print 'Averaging weights'\n",
            "+    print('Averaging weights')\n",
            "     parser.model.average_weights()\n",
            " \n",
            " def read_pos(loc):\n",
            "@@ -475,9 +475,9 @@\n",
            "         words = DefaultList(''); tags = DefaultList('')\n",
            "         heads = [None]; labels = [None]\n",
            "         for i, (word, pos, head, label) in enumerate(lines):\n",
            "-            words.append(intern(word))\n",
            "+            words.append(sys.intern(word))\n",
            "             #words.append(intern(normalize(word)))\n",
            "-            tags.append(intern(pos))\n",
            "+            tags.append(sys.intern(pos))\n",
            "             heads.append(int(head) + 1 if head != '-1' else len(lines) + 1)\n",
            "             labels.append(label)\n",
            "         pad_tokens(words); pad_tokens(tags)\n",
            "@@ -511,8 +511,8 @@\n",
            "                 c += 1\n",
            "             t += 1\n",
            "     t2 = time.time()\n",
            "-    print 'Parsing took %0.3f ms' % ((t2-t1)*1000.0)\n",
            "-    print c, t, float(c)/t\n",
            "+    print('Parsing took %0.3f ms' % ((t2-t1)*1000.0))\n",
            "+    print(c, t, float(c)/t)\n",
            " \n",
            " \n",
            " if __name__ == '__main__':\n",
            "RefactoringTool: Files that were modified:\n",
            "RefactoringTool: gistfile1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open ('gistfile1.py', 'r') as file:\n",
        "  print(file.read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-79WU4u4j6UW",
        "outputId": "887adaf7-249d-47f6-f7cb-6319f54e645f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"\"\"A simple implementation of a greedy transition-based parser. Released under BSD license.\"\"\"\n",
            "from os import path\n",
            "import os\n",
            "import sys\n",
            "from collections import defaultdict\n",
            "import random\n",
            "import time\n",
            "import pickle\n",
            "\n",
            "SHIFT = 0; RIGHT = 1; LEFT = 2;\n",
            "MOVES = (SHIFT, RIGHT, LEFT)\n",
            "START = ['-START-', '-START2-']\n",
            "END = ['-END-', '-END2-']\n",
            "\n",
            "\n",
            "class DefaultList(list):\n",
            "    \"\"\"A list that returns a default value if index out of bounds.\"\"\"\n",
            "    def __init__(self, default=None):\n",
            "        self.default = default\n",
            "        list.__init__(self)\n",
            "\n",
            "    def __getitem__(self, index):\n",
            "        try:\n",
            "            return list.__getitem__(self, index)\n",
            "        except IndexError:\n",
            "            return self.default\n",
            "\n",
            "\n",
            "class Parse(object):\n",
            "    def __init__(self, n):\n",
            "        self.n = n\n",
            "        self.heads = [None] * (n-1)\n",
            "        self.labels = [None] * (n-1)\n",
            "        self.lefts = []\n",
            "        self.rights = []\n",
            "        for i in range(n+1):\n",
            "            self.lefts.append(DefaultList(0))\n",
            "            self.rights.append(DefaultList(0))\n",
            "\n",
            "    def add(self, head, child, label=None):\n",
            "        self.heads[child] = head\n",
            "        self.labels[child] = label\n",
            "        if child < head:\n",
            "            self.lefts[head].append(child)\n",
            "        else:\n",
            "            self.rights[head].append(child)\n",
            "\n",
            "\n",
            "class Parser(object):\n",
            "    def __init__(self, load=True):\n",
            "        model_dir = os.path.dirname(__file__)\n",
            "        self.model = Perceptron(MOVES)\n",
            "        if load:\n",
            "            self.model.load(path.join(model_dir, 'parser.pickle'))\n",
            "        self.tagger = PerceptronTagger(load=load)\n",
            "        self.confusion_matrix = defaultdict(lambda: defaultdict(int))\n",
            "\n",
            "    def save(self):\n",
            "        self.model.save(path.join(os.path.dirname(__file__), 'parser.pickle'))\n",
            "        self.tagger.save()\n",
            "    \n",
            "    def parse(self, words):\n",
            "        n = len(words)\n",
            "        i = 2; stack = [1]; parse = Parse(n)\n",
            "        tags = self.tagger.tag(words)\n",
            "        while stack or (i+1) < n:\n",
            "            features = extract_features(words, tags, i, n, stack, parse)\n",
            "            scores = self.model.score(features)\n",
            "            valid_moves = get_valid_moves(i, n, len(stack))\n",
            "            guess = max(valid_moves, key=lambda move: scores[move])\n",
            "            i = transition(guess, i, stack, parse)\n",
            "        return tags, parse.heads\n",
            "\n",
            "    def train_one(self, itn, words, gold_tags, gold_heads):\n",
            "        n = len(words)\n",
            "        i = 2; stack = [1]; parse = Parse(n)\n",
            "        tags = self.tagger.tag(words)\n",
            "        while stack or (i + 1) < n:\n",
            "            features = extract_features(words, tags, i, n, stack, parse)\n",
            "            scores = self.model.score(features)\n",
            "            valid_moves = get_valid_moves(i, n, len(stack))\n",
            "            gold_moves = get_gold_moves(i, n, stack, parse.heads, gold_heads)\n",
            "            guess = max(valid_moves, key=lambda move: scores[move])\n",
            "            assert gold_moves\n",
            "            best = max(gold_moves, key=lambda move: scores[move])\n",
            "            self.model.update(best, guess, features)\n",
            "            i = transition(guess, i, stack, parse)\n",
            "            self.confusion_matrix[best][guess] += 1\n",
            "        return len([i for i in range(n-1) if parse.heads[i] == gold_heads[i]])\n",
            "\n",
            "\n",
            "def transition(move, i, stack, parse):\n",
            "    if move == SHIFT:\n",
            "        stack.append(i)\n",
            "        return i + 1\n",
            "    elif move == RIGHT:\n",
            "        parse.add(stack[-2], stack.pop())\n",
            "        return i\n",
            "    elif move == LEFT:\n",
            "        parse.add(i, stack.pop())\n",
            "        return i\n",
            "    assert move in MOVES\n",
            "\n",
            "\n",
            "def get_valid_moves(i, n, stack_depth):\n",
            "    moves = []\n",
            "    if (i+1) < n:\n",
            "        moves.append(SHIFT)\n",
            "    if stack_depth >= 2:\n",
            "        moves.append(RIGHT)\n",
            "    if stack_depth >= 1:\n",
            "        moves.append(LEFT)\n",
            "    return moves\n",
            "\n",
            "\n",
            "def get_gold_moves(n0, n, stack, heads, gold):\n",
            "    def deps_between(target, others, gold):\n",
            "        for word in others:\n",
            "            if gold[word] == target or gold[target] == word:\n",
            "                return True\n",
            "        return False\n",
            "\n",
            "    valid = get_valid_moves(n0, n, len(stack))\n",
            "    if not stack or (SHIFT in valid and gold[n0] == stack[-1]):\n",
            "        return [SHIFT]\n",
            "    if gold[stack[-1]] == n0:\n",
            "        return [LEFT]\n",
            "    costly = set([m for m in MOVES if m not in valid])\n",
            "    # If the word behind s0 is its gold head, Left is incorrect\n",
            "    if len(stack) >= 2 and gold[stack[-1]] == stack[-2]:\n",
            "        costly.add(LEFT)\n",
            "    # If there are any dependencies between n0 and the stack,\n",
            "    # pushing n0 will lose them.\n",
            "    if SHIFT not in costly and deps_between(n0, stack, gold):\n",
            "        costly.add(SHIFT)\n",
            "    # If there are any dependencies between s0 and the buffer, popping\n",
            "    # s0 will lose them.\n",
            "    if deps_between(stack[-1], list(range(n0+1, n-1)), gold):\n",
            "        costly.add(LEFT)\n",
            "        costly.add(RIGHT)\n",
            "    return [m for m in MOVES if m not in costly]\n",
            "\n",
            "\n",
            "def extract_features(words, tags, n0, n, stack, parse):\n",
            "    def get_stack_context(depth, stack, data):\n",
            "        if depth >= 3:\n",
            "            return data[stack[-1]], data[stack[-2]], data[stack[-3]]\n",
            "        elif depth >= 2:\n",
            "            return data[stack[-1]], data[stack[-2]], ''\n",
            "        elif depth == 1:\n",
            "            return data[stack[-1]], '', ''\n",
            "        else:\n",
            "            return '', '', ''\n",
            "\n",
            "    def get_buffer_context(i, n, data):\n",
            "        if i + 1 >= n:\n",
            "            return data[i], '', ''\n",
            "        elif i + 2 >= n:\n",
            "            return data[i], data[i + 1], ''\n",
            "        else:\n",
            "            return data[i], data[i + 1], data[i + 2]\n",
            "\n",
            "    def get_parse_context(word, deps, data):\n",
            "        if word == -1:\n",
            "            return 0, '', ''\n",
            "        deps = deps[word]\n",
            "        valency = len(deps)\n",
            "        if not valency:\n",
            "            return 0, '', ''\n",
            "        elif valency == 1:\n",
            "            return 1, data[deps[-1]], ''\n",
            "        else:\n",
            "            return valency, data[deps[-1]], data[deps[-2]]\n",
            "\n",
            "    features = {}\n",
            "    # Set up the context pieces --- the word (W) and tag (T) of:\n",
            "    # S0-2: Top three words on the stack\n",
            "    # N0-2: First three words of the buffer\n",
            "    # n0b1, n0b2: Two leftmost children of the first word of the buffer\n",
            "    # s0b1, s0b2: Two leftmost children of the top word of the stack\n",
            "    # s0f1, s0f2: Two rightmost children of the top word of the stack\n",
            "\n",
            "    depth = len(stack)\n",
            "    s0 = stack[-1] if depth else -1\n",
            "\n",
            "    Ws0, Ws1, Ws2 = get_stack_context(depth, stack, words)\n",
            "    Ts0, Ts1, Ts2 = get_stack_context(depth, stack, tags)\n",
            "   \n",
            "    Wn0, Wn1, Wn2 = get_buffer_context(n0, n, words)\n",
            "    Tn0, Tn1, Tn2 = get_buffer_context(n0, n, tags)\n",
            "    \n",
            "    Vn0b, Wn0b1, Wn0b2 = get_parse_context(n0, parse.lefts, words)\n",
            "    Vn0b, Tn0b1, Tn0b2 = get_parse_context(n0, parse.lefts, tags)\n",
            "    \n",
            "    Vn0f, Wn0f1, Wn0f2 = get_parse_context(n0, parse.rights, words)\n",
            "    _, Tn0f1, Tn0f2 = get_parse_context(n0, parse.rights, tags)\n",
            "  \n",
            "    Vs0b, Ws0b1, Ws0b2 = get_parse_context(s0, parse.lefts, words)\n",
            "    _, Ts0b1, Ts0b2 = get_parse_context(s0, parse.lefts, tags)\n",
            "\n",
            "    Vs0f, Ws0f1, Ws0f2 = get_parse_context(s0, parse.rights, words)\n",
            "    _, Ts0f1, Ts0f2 = get_parse_context(s0, parse.rights, tags)\n",
            "    \n",
            "    # Cap numeric features at 5? \n",
            "    # String-distance\n",
            "    Ds0n0 = min((n0 - s0, 5)) if s0 != 0 else 0\n",
            "\n",
            "    features['bias'] = 1\n",
            "    # Add word and tag unigrams\n",
            "    for w in (Wn0, Wn1, Wn2, Ws0, Ws1, Ws2, Wn0b1, Wn0b2, Ws0b1, Ws0b2, Ws0f1, Ws0f2):\n",
            "        if w:\n",
            "            features['w=%s' % w] = 1\n",
            "    for t in (Tn0, Tn1, Tn2, Ts0, Ts1, Ts2, Tn0b1, Tn0b2, Ts0b1, Ts0b2, Ts0f1, Ts0f2):\n",
            "        if t:\n",
            "            features['t=%s' % t] = 1\n",
            "\n",
            "    # Add word/tag pairs\n",
            "    for i, (w, t) in enumerate(((Wn0, Tn0), (Wn1, Tn1), (Wn2, Tn2), (Ws0, Ts0))):\n",
            "        if w or t:\n",
            "            features['%d w=%s, t=%s' % (i, w, t)] = 1\n",
            "\n",
            "    # Add some bigrams\n",
            "    features['s0w=%s,  n0w=%s' % (Ws0, Wn0)] = 1\n",
            "    features['wn0tn0-ws0 %s/%s %s' % (Wn0, Tn0, Ws0)] = 1\n",
            "    features['wn0tn0-ts0 %s/%s %s' % (Wn0, Tn0, Ts0)] = 1\n",
            "    features['ws0ts0-wn0 %s/%s %s' % (Ws0, Ts0, Wn0)] = 1\n",
            "    features['ws0-ts0 tn0 %s/%s %s' % (Ws0, Ts0, Tn0)] = 1\n",
            "    features['wt-wt %s/%s %s/%s' % (Ws0, Ts0, Wn0, Tn0)] = 1\n",
            "    features['tt s0=%s n0=%s' % (Ts0, Tn0)] = 1\n",
            "    features['tt n0=%s n1=%s' % (Tn0, Tn1)] = 1\n",
            "\n",
            "    # Add some tag trigrams\n",
            "    trigrams = ((Tn0, Tn1, Tn2), (Ts0, Tn0, Tn1), (Ts0, Ts1, Tn0), \n",
            "                (Ts0, Ts0f1, Tn0), (Ts0, Ts0f1, Tn0), (Ts0, Tn0, Tn0b1),\n",
            "                (Ts0, Ts0b1, Ts0b2), (Ts0, Ts0f1, Ts0f2), (Tn0, Tn0b1, Tn0b2),\n",
            "                (Ts0, Ts1, Ts1))\n",
            "    for i, (t1, t2, t3) in enumerate(trigrams):\n",
            "        if t1 or t2 or t3:\n",
            "            features['ttt-%d %s %s %s' % (i, t1, t2, t3)] = 1\n",
            "\n",
            "    # Add some valency and distance features\n",
            "    vw = ((Ws0, Vs0f), (Ws0, Vs0b), (Wn0, Vn0b))\n",
            "    vt = ((Ts0, Vs0f), (Ts0, Vs0b), (Tn0, Vn0b))\n",
            "    d = ((Ws0, Ds0n0), (Wn0, Ds0n0), (Ts0, Ds0n0), (Tn0, Ds0n0),\n",
            "         ('t' + Tn0+Ts0, Ds0n0), ('w' + Wn0+Ws0, Ds0n0))\n",
            "    for i, (w_t, v_d) in enumerate(vw + vt + d):\n",
            "        if w_t or v_d:\n",
            "            features['val/d-%d %s %d' % (i, w_t, v_d)] = 1\n",
            "    return features\n",
            "\n",
            "\n",
            "class Perceptron(object):\n",
            "    def __init__(self, classes=None):\n",
            "        # Each feature gets its own weight vector, so weights is a dict-of-arrays\n",
            "        self.classes = classes\n",
            "        self.weights = {}\n",
            "        # The accumulated values, for the averaging. These will be keyed by\n",
            "        # feature/clas tuples\n",
            "        self._totals = defaultdict(int)\n",
            "        # The last time the feature was changed, for the averaging. Also\n",
            "        # keyed by feature/clas tuples\n",
            "        # (tstamps is short for timestamps)\n",
            "        self._tstamps = defaultdict(int)\n",
            "        # Number of instances seen\n",
            "        self.i = 0\n",
            "\n",
            "    def predict(self, features):\n",
            "        '''Dot-product the features and current weights and return the best class.'''\n",
            "        scores = self.score(features)\n",
            "        # Do a secondary alphabetic sort, for stability\n",
            "        return max(self.classes, key=lambda clas: (scores[clas], clas))\n",
            "\n",
            "    def score(self, features):\n",
            "        all_weights = self.weights\n",
            "        scores = dict((clas, 0) for clas in self.classes)\n",
            "        for feat, value in list(features.items()):\n",
            "            if value == 0:\n",
            "                continue\n",
            "            if feat not in all_weights:\n",
            "                continue\n",
            "            weights = all_weights[feat]\n",
            "            for clas, weight in list(weights.items()):\n",
            "                scores[clas] += value * weight\n",
            "        return scores\n",
            "\n",
            "    def update(self, truth, guess, features):       \n",
            "        def upd_feat(c, f, w, v):\n",
            "            param = (f, c)\n",
            "            self._totals[param] += (self.i - self._tstamps[param]) * w\n",
            "            self._tstamps[param] = self.i\n",
            "            self.weights[f][c] = w + v\n",
            "\n",
            "        self.i += 1\n",
            "        if truth == guess:\n",
            "            return None\n",
            "        for f in features:\n",
            "            weights = self.weights.setdefault(f, {})\n",
            "            upd_feat(truth, f, weights.get(truth, 0.0), 1.0)\n",
            "            upd_feat(guess, f, weights.get(guess, 0.0), -1.0)\n",
            "\n",
            "    def average_weights(self):\n",
            "        for feat, weights in list(self.weights.items()):\n",
            "            new_feat_weights = {}\n",
            "            for clas, weight in list(weights.items()):\n",
            "                param = (feat, clas)\n",
            "                total = self._totals[param]\n",
            "                total += (self.i - self._tstamps[param]) * weight\n",
            "                averaged = round(total / float(self.i), 3)\n",
            "                if averaged:\n",
            "                    new_feat_weights[clas] = averaged\n",
            "            self.weights[feat] = new_feat_weights\n",
            "\n",
            "    def save(self, path):\n",
            "        print(\"Saving model to %s\" % path)\n",
            "        pickle.dump(self.weights, open(path, 'w'))\n",
            "\n",
            "    def load(self, path):\n",
            "        self.weights = pickle.load(open(path))\n",
            "\n",
            "\n",
            "class PerceptronTagger(object):\n",
            "    '''Greedy Averaged Perceptron tagger'''\n",
            "    model_loc = os.path.join(os.path.dirname(__file__), 'tagger.pickle')\n",
            "    def __init__(self, classes=None, load=True):\n",
            "        self.tagdict = {}\n",
            "        if classes:\n",
            "            self.classes = classes\n",
            "        else:\n",
            "            self.classes = set()\n",
            "        self.model = Perceptron(self.classes)\n",
            "        if load:\n",
            "            self.load(PerceptronTagger.model_loc)\n",
            "\n",
            "    def tag(self, words, tokenize=True):\n",
            "        prev, prev2 = START\n",
            "        tags = DefaultList('') \n",
            "        context = START + [self._normalize(w) for w in words] + END\n",
            "        for i, word in enumerate(words):\n",
            "            tag = self.tagdict.get(word)\n",
            "            if not tag:\n",
            "                features = self._get_features(i, word, context, prev, prev2)\n",
            "                tag = self.model.predict(features)\n",
            "            tags.append(tag)\n",
            "            prev2 = prev; prev = tag\n",
            "        return tags\n",
            "\n",
            "    def start_training(self, sentences):\n",
            "        self._make_tagdict(sentences)\n",
            "        self.model = Perceptron(self.classes)\n",
            "\n",
            "    def train(self, sentences, save_loc=None, nr_iter=5):\n",
            "        '''Train a model from sentences, and save it at save_loc. nr_iter\n",
            "        controls the number of Perceptron training iterations.'''\n",
            "        self.start_training(sentences)\n",
            "        for iter_ in range(nr_iter):\n",
            "            for words, tags in sentences:\n",
            "                self.train_one(words, tags)\n",
            "            random.shuffle(sentences)\n",
            "        self.end_training(save_loc)\n",
            "\n",
            "    def save(self):\n",
            "        # Pickle as a binary file\n",
            "        pickle.dump((self.model.weights, self.tagdict, self.classes),\n",
            "                    open(PerceptronTagger.model_loc, 'wb'), -1)\n",
            "\n",
            "    def train_one(self, words, tags):\n",
            "        prev, prev2 = START\n",
            "        context = START + [self._normalize(w) for w in words] + END\n",
            "        for i, word in enumerate(words):\n",
            "            guess = self.tagdict.get(word)\n",
            "            if not guess:\n",
            "                feats = self._get_features(i, word, context, prev, prev2)\n",
            "                guess = self.model.predict(feats)\n",
            "                self.model.update(tags[i], guess, feats)\n",
            "            prev2 = prev; prev = guess\n",
            "\n",
            "    def load(self, loc):\n",
            "        w_td_c = pickle.load(open(loc, 'rb'))\n",
            "        self.model.weights, self.tagdict, self.classes = w_td_c\n",
            "        self.model.classes = self.classes\n",
            "\n",
            "    def _normalize(self, word):\n",
            "        if '-' in word and word[0] != '-':\n",
            "            return '!HYPHEN'\n",
            "        elif word.isdigit() and len(word) == 4:\n",
            "            return '!YEAR'\n",
            "        elif word[0].isdigit():\n",
            "            return '!DIGITS'\n",
            "        else:\n",
            "            return word.lower()\n",
            "\n",
            "    def _get_features(self, i, word, context, prev, prev2):\n",
            "        '''Map tokens into a feature representation, implemented as a\n",
            "        {hashable: float} dict. If the features change, a new model must be\n",
            "        trained.'''\n",
            "        def add(name, *args):\n",
            "            features[' '.join((name,) + tuple(args))] += 1\n",
            "\n",
            "        i += len(START)\n",
            "        features = defaultdict(int)\n",
            "        # It's useful to have a constant feature, which acts sort of like a prior\n",
            "        add('bias')\n",
            "        add('i suffix', word[-3:])\n",
            "        add('i pref1', word[0])\n",
            "        add('i-1 tag', prev)\n",
            "        add('i-2 tag', prev2)\n",
            "        add('i tag+i-2 tag', prev, prev2)\n",
            "        add('i word', context[i])\n",
            "        add('i-1 tag+i word', prev, context[i])\n",
            "        add('i-1 word', context[i-1])\n",
            "        add('i-1 suffix', context[i-1][-3:])\n",
            "        add('i-2 word', context[i-2])\n",
            "        add('i+1 word', context[i+1])\n",
            "        add('i+1 suffix', context[i+1][-3:])\n",
            "        add('i+2 word', context[i+2])\n",
            "        return features\n",
            "\n",
            "    def _make_tagdict(self, sentences):\n",
            "        '''Make a tag dictionary for single-tag words.'''\n",
            "        counts = defaultdict(lambda: defaultdict(int))\n",
            "        for sent in sentences:\n",
            "            for word, tag in zip(sent[0], sent[1]):\n",
            "                counts[word][tag] += 1\n",
            "                self.classes.add(tag)\n",
            "        freq_thresh = 20\n",
            "        ambiguity_thresh = 0.97\n",
            "        for word, tag_freqs in list(counts.items()):\n",
            "            tag, mode = max(list(tag_freqs.items()), key=lambda item: item[1])\n",
            "            n = sum(tag_freqs.values())\n",
            "            # Don't add rare words to the tag dictionary\n",
            "            # Only add quite unambiguous words\n",
            "            if n >= freq_thresh and (float(mode) / n) >= ambiguity_thresh:\n",
            "                self.tagdict[word] = tag\n",
            "\n",
            "def _pc(n, d):\n",
            "    return (float(n) / d) * 100\n",
            "\n",
            "\n",
            "def train(parser, sentences, nr_iter):\n",
            "    parser.tagger.start_training(sentences)\n",
            "    for itn in range(nr_iter):\n",
            "        corr = 0; total = 0\n",
            "        random.shuffle(sentences)\n",
            "        for words, gold_tags, gold_parse, gold_label in sentences:\n",
            "            corr += parser.train_one(itn, words, gold_tags, gold_parse)\n",
            "            if itn < 5:\n",
            "                parser.tagger.train_one(words, gold_tags)\n",
            "            total += len(words)\n",
            "        print(itn, '%.3f' % (float(corr) / float(total)))\n",
            "        if itn == 4:\n",
            "            parser.tagger.model.average_weights()\n",
            "    print('Averaging weights')\n",
            "    parser.model.average_weights()\n",
            "\n",
            "def read_pos(loc):\n",
            "    for line in open(loc):\n",
            "        if not line.strip():\n",
            "            continue\n",
            "        words = DefaultList('')\n",
            "        tags = DefaultList('')\n",
            "        for token in line.split():\n",
            "            if not token:\n",
            "                continue\n",
            "            word, tag = token.rsplit('/', 1)\n",
            "            #words.append(normalize(word))\n",
            "            words.append(word)\n",
            "            tags.append(tag)\n",
            "        pad_tokens(words); pad_tokens(tags)\n",
            "        yield words, tags\n",
            "\n",
            "\n",
            "def read_conll(loc):\n",
            "    for sent_str in open(loc).read().strip().split('\\n\\n'):\n",
            "        lines = [line.split() for line in sent_str.split('\\n')]\n",
            "        words = DefaultList(''); tags = DefaultList('')\n",
            "        heads = [None]; labels = [None]\n",
            "        for i, (word, pos, head, label) in enumerate(lines):\n",
            "            words.append(sys.intern(word))\n",
            "            #words.append(intern(normalize(word)))\n",
            "            tags.append(sys.intern(pos))\n",
            "            heads.append(int(head) + 1 if head != '-1' else len(lines) + 1)\n",
            "            labels.append(label)\n",
            "        pad_tokens(words); pad_tokens(tags)\n",
            "        yield words, tags, heads, labels\n",
            "\n",
            "\n",
            "def pad_tokens(tokens):\n",
            "    tokens.insert(0, '<start>')\n",
            "    tokens.append('ROOT')\n",
            "\n",
            "\n",
            "def main(model_dir, train_loc, heldout_in, heldout_gold):\n",
            "    if not os.path.exists(model_dir):\n",
            "        os.mkdir(model_dir)\n",
            "\n",
            "    input_sents = list(read_pos(heldout_in))\n",
            "    parser = Parser(load=False)\n",
            "    sentences = list(read_conll(train_loc))\n",
            "    train(parser, sentences, nr_iter=15)\n",
            "    parser.save()\n",
            "    c = 0\n",
            "    t = 0\n",
            "    gold_sents = list(read_conll(heldout_gold))\n",
            "    t1 = time.time()\n",
            "    for (words, tags), (_, _, gold_heads, gold_labels) in zip(input_sents, gold_sents):\n",
            "        _, heads = parser.parse(words)\n",
            "        for i, w in list(enumerate(words))[1:-1]:\n",
            "            if gold_labels[i] in ('P', 'punct'):\n",
            "                continue\n",
            "            if heads[i] == gold_heads[i]:\n",
            "                c += 1\n",
            "            t += 1\n",
            "    t2 = time.time()\n",
            "    print('Parsing took %0.3f ms' % ((t2-t1)*1000.0))\n",
            "    print(c, t, float(c)/t)\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    main(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('gistfile1.py')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "_D2MI100kLt3",
        "outputId": "ca6898d3-e564-4764-9298-a3b3c1c0dc68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2d625b86-f862-49f9-bdb2-ccddc876948e\", \"gistfile1.py\", 18500)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !wget https://github.com/UniversalDependencies/UD_German-HDT.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiSC35IHnBaD",
        "outputId": "a585f841-c79a-48ec-d437-842abfb998b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-12 08:06:41--  https://github.com/UniversalDependencies/UD_German-HDT.git\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/UniversalDependencies/UD_German-HDT [following]\n",
            "--2024-08-12 08:06:41--  https://github.com/UniversalDependencies/UD_German-HDT\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: UD_German-HDT.git\n",
            "\n",
            "UD_German-HDT.git       [ <=>                ] 329.45K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2024-08-12 08:06:41 (4.52 MB/s) - UD_German-HDT.git saved [337354]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from collections import defaultdict\n",
        "#https://raw.githubusercontent.com/UniversalDependencies/UD_German-HDT/dev/de_hdt-ud-dev.conll\n",
        "\n",
        "\n",
        "# MHD Bashir Kabbani 3017375\n",
        "# Vialli Tem Ndumbe 2911472\n",
        "\n",
        "# Step 1: Download the treebank file\n",
        "url = 'https://github.com/UniversalDependencies/UD_German-HDT.git'\n",
        "response = requests.get(url)\n",
        "file_content = response.text\n",
        "\n",
        "# Step 2: Parse the file content\n",
        "def parse_conllu(file_content):\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    sentence_meta = {}\n",
        "    for line in file_content.split('\\n'):\n",
        "        if line.startswith('#'):\n",
        "            if line.startswith('# sent_id'):\n",
        "                sentence_meta['id'] = line.split('=')[-1].strip()\n",
        "            elif line.startswith('# text'):\n",
        "                sentence_meta['text'] = line.split('=')[-1].strip()\n",
        "        elif line.strip() == '':\n",
        "            if sentence:\n",
        "                sentences.append((sentence_meta, sentence))\n",
        "                sentence = []\n",
        "                sentence_meta = {}\n",
        "        else:\n",
        "            sentence.append(line.split('\\t'))\n",
        "    return sentences\n",
        "\n",
        "# Step 3: Identify non-projective trees\n",
        "def is_non_projective(sentence):\n",
        "    arcs = []\n",
        "    for token in sentence:\n",
        "        if len(token) < 8:\n",
        "            continue  # Skip tokens with fewer than 8 elements\n",
        "        try:\n",
        "            head = int(token[6])\n",
        "            dependent = int(token[0])\n",
        "        except ValueError:\n",
        "            continue  # Skip tokens where head or id is not an integer\n",
        "        if head == 0:  # Skip root\n",
        "            continue\n",
        "        if head > dependent:\n",
        "            head, dependent = dependent, head\n",
        "        arcs.append((head, dependent))\n",
        "\n",
        "    for i in range(len(arcs)):\n",
        "        for j in range(i + 1, len(arcs)):\n",
        "            (h1, d1) = arcs[i]\n",
        "            (h2, d2) = arcs[j]\n",
        "            if (h1 < h2 < d1 < d2) or (h2 < h1 < d2 < d1):\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# Step 4: Extract non-projective sentences and their identifiers\n",
        "def extract_non_projective_sentences(file_content):\n",
        "    sentences = parse_conllu(file_content)\n",
        "    non_projective_sentences = []\n",
        "    for sentence_meta, sentence in sentences:\n",
        "        if is_non_projective(sentence):\n",
        "            non_projective_sentences.append((sentence_meta['id'], sentence_meta['text'], sentence))\n",
        "    return non_projective_sentences\n",
        "\n",
        "# Step 5: Main execution\n",
        "non_projective_sentences = extract_non_projective_sentences(file_content)\n",
        "print(\"Non-projective sentences:\")\n",
        "for sentence_id, sentence_text, sentence in non_projective_sentences:\n",
        "    print(f\"ID: {sentence_id}\")\n",
        "    print(f\"Text: {sentence_text}\")\n",
        "    print()\n",
        "\n",
        "# Step 5: Count non-projective sentences\n",
        "def count_non_projective_sentences(non_projective_sentences):\n",
        "    return len(non_projective_sentences)\n",
        "\n",
        "# Print the count of non-projective sentences\n",
        "total_non_projective = count_non_projective_sentences(non_projective_sentences)\n",
        "print(f\"Total number of non-projective sentences: {total_non_projective}\")\n",
        "\n",
        "# Exercise 2: Reordered list of words to make the tree projective\n",
        "def create_dependency_graph(sentence):\n",
        "    graph = defaultdict(list)\n",
        "    words = {}\n",
        "    for token in sentence:\n",
        "        if len(token) < 8:\n",
        "            continue\n",
        "        try:\n",
        "            head = int(token[6])\n",
        "            dependent = int(token[0])\n",
        "            word = token[1]\n",
        "        except ValueError:\n",
        "            continue  # Skip tokens where head or id is not an integer\n",
        "        graph[head].append(dependent)\n",
        "        words[dependent] = word\n",
        "    return graph, words\n",
        "\n",
        "def inorder_traversal(node, graph, words, result):\n",
        "    if node not in graph:\n",
        "        return\n",
        "    children = sorted(graph[node])\n",
        "    for child in children:\n",
        "        inorder_traversal(child, graph, words, result)\n",
        "    if node in words:\n",
        "        result.append(words[node])\n",
        "\n",
        "def make_projective(sentence):\n",
        "    graph, words = create_dependency_graph(sentence)\n",
        "    result = []\n",
        "    inorder_traversal(0, graph, words, result)\n",
        "    return result\n",
        "\n",
        "print(\"\\nExercise 2: Reordered sentences to make the tree projective:\")\n",
        "for sentence_id, sentence_text, sentence in non_projective_sentences:\n",
        "    projective_order = make_projective(sentence)\n",
        "    print(f\"ID: {sentence_id}\")\n",
        "    print(f\"Original Text: {sentence_text}\")\n",
        "    print(\"Projective Order:\", \" \".join(projective_order))\n",
        "    print()\n",
        "\n",
        "# Exercise 3: Analysis of syntactic construction types\n",
        "def analyze_construction_types(non_projective_sentences, num_sentences=50):\n",
        "    construction_types = {\n",
        "        'wh-movement/fronting': [],\n",
        "        'extraposition': [],\n",
        "        'parentheticals/insertions': []\n",
        "    }\n",
        "\n",
        "    for sentence_id, sentence_text, sentence in non_projective_sentences[:num_sentences]:\n",
        "        for token in sentence:\n",
        "            if len(token) >= 8:\n",
        "                construction = token[7]\n",
        "                if construction.startswith('wh'):\n",
        "                    construction_types['wh-movement/fronting'].append((sentence_id, sentence_text))\n",
        "                elif construction.startswith('extr'):\n",
        "                    construction_types['extraposition'].append((sentence_id, sentence_text))\n",
        "                elif construction.startswith('parataxis'):\n",
        "                    construction_types['parentheticals/insertions'].append((sentence_id, sentence_text))\n",
        "\n",
        "    return construction_types\n",
        "\n",
        "print(\"\\nExercise 3: Analysis of Non-Projective Sentences:\")\n",
        "construction_types = analyze_construction_types(non_projective_sentences)\n",
        "\n",
        "print(\"\\n## Most frequent syntactic construction type: Wh-movement/Fronting\")\n",
        "print(\"Examples:\")\n",
        "for sent_id, sent_text in construction_types['wh-movement/fronting']:\n",
        "    print(f\"- ID: {sent_id}, Text: {sent_text}\")\n",
        "\n",
        "print(\"\\n### Other construction types:\")\n",
        "print(\"1. Extraposition\")\n",
        "print(\"Examples:\")\n",
        "for sent_id, sent_text in construction_types['extraposition']:\n",
        "    print(f\"- ID: {sent_id}, Text: {sent_text}\")\n",
        "\n",
        "print(\"\\n2. Parentheticals/Insertions\")\n",
        "print(\"Examples:\")\n",
        "for sent_id, sent_text in construction_types['parentheticals/insertions']:\n",
        "    print(f\"- ID: {sent_id}, Text: {sent_text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUa5J54_uvVz",
        "outputId": "2c792afd-0723-4453-f7c5-73657f95929e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-projective sentences:\n",
            "Total number of non-projective sentences: 0\n",
            "\n",
            "Exercise 2: Reordered sentences to make the tree projective:\n",
            "\n",
            "Exercise 3: Analysis of Non-Projective Sentences:\n",
            "\n",
            "## Most frequent syntactic construction type: Wh-movement/Fronting\n",
            "Examples:\n",
            "\n",
            "### Other construction types:\n",
            "1. Extraposition\n",
            "Examples:\n",
            "\n",
            "2. Parentheticals/Insertions\n",
            "Examples:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy"
      ],
      "metadata": {
        "id": "3QI8oehSvK4w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8e7ab53-37d5-41d9-9d16-c2f8cd6ee7c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define constants\n",
        "SHIFT = 0\n",
        "RIGHT = 1\n",
        "LEFT = 2\n",
        "MOVES = (SHIFT, RIGHT, LEFT)\n",
        "\n",
        "class DefaultList(list):\n",
        "    \"\"\"A list that returns a default value if index out of bounds.\"\"\"\n",
        "    def __init__(self, default=None):\n",
        "        self.default = default\n",
        "        list.__init__(self)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        try:\n",
        "            return list.__getitem__(self, index)\n",
        "        except IndexError:\n",
        "            return self.default\n",
        "\n",
        "class Parse(object):\n",
        "    def __init__(self, n):\n",
        "        self.n = n\n",
        "        self.heads = [None] * (n-1)\n",
        "        self.labels = [None] * (n-1)\n",
        "        self.lefts = []\n",
        "        self.rights = []\n",
        "        for i in range(n+1):\n",
        "            self.lefts.append(DefaultList(0))\n",
        "            self.rights.append(DefaultList(0))\n",
        "\n",
        "    def add(self, head, child, label=None):\n",
        "        self.heads[child] = head\n",
        "        self.labels[child] = label\n",
        "        if child < head:\n",
        "            self.lefts[head].append(child)\n",
        "        else:\n",
        "            self.rights[head].append(child)\n",
        "\n",
        "class Parser(object):\n",
        "    def __init__(self, load=True):\n",
        "        self.model = Perceptron(MOVES)\n",
        "        if load:\n",
        "            self.model.load('/content/drive/MyDrive/parser.pickle')\n",
        "        self.tagger = PerceptronTagger(load=load)\n",
        "        self.confusion_matrix = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    def save(self):\n",
        "        self.model.save('/content/drive/MyDrive/parser.pickle')\n",
        "        self.tagger.save()\n",
        "\n",
        "    def parse(self, words):\n",
        "        n = len(words)\n",
        "        i = 2\n",
        "        stack = [1]\n",
        "        parse = Parse(n)\n",
        "        tags = self.tagger.tag(words)\n",
        "        while stack or (i + 1) < n:\n",
        "            features = extract_features(words, tags, i, n, stack, parse)\n",
        "            scores = self.model.score(features)\n",
        "            valid_moves = get_valid_moves(i, n, len(stack))\n",
        "            guess = max(valid_moves, key=lambda move: scores[move])\n",
        "            i = transition(guess, i, stack, parse)\n",
        "        return tags, parse.heads\n",
        "\n",
        "    def train_one(self, itn, words, gold_tags, gold_heads):\n",
        "        n = len(words)\n",
        "        i = 2\n",
        "        stack = [1]\n",
        "        parse = Parse(n)\n",
        "        tags = self.tagger.tag(words)\n",
        "        while stack or (i + 1) < n:\n",
        "            features = extract_features(words, tags, i, n, stack, parse)\n",
        "            scores = self.model.score(features)\n",
        "            valid_moves = get_valid_moves(i, n, len(stack))\n",
        "            gold_moves = get_gold_moves(i, n, stack, parse.heads, gold_heads)\n",
        "            guess = max(valid_moves, key=lambda move: scores[move])\n",
        "            assert gold_moves\n",
        "            best = max(gold_moves, key=lambda move: scores[move])\n",
        "            self.model.update(best, guess, features)\n",
        "            i = transition(guess, i, stack, parse)\n",
        "            self.confusion_matrix[best][guess] += 1\n",
        "        return len([i for i in range(n-1) if parse.heads[i] == gold_heads[i]])\n",
        "\n",
        "def transition(move, i, stack, parse):\n",
        "    if move == SHIFT:\n",
        "        stack.append(i)\n",
        "        return i + 1\n",
        "    elif move == RIGHT:\n",
        "        parse.add(stack[-2], stack.pop())\n",
        "        return i\n",
        "    elif move == LEFT:\n",
        "        parse.add(i, stack.pop())\n",
        "        return i\n",
        "    assert move in MOVES\n",
        "\n",
        "def get_valid_moves(i, n, stack_depth):\n",
        "    moves = []\n",
        "    if (i + 1) < n:\n",
        "        moves.append(SHIFT)\n",
        "    if stack_depth >= 2:\n",
        "        moves.append(RIGHT)\n",
        "    if stack_depth >= 1:\n",
        "        moves.append(LEFT)\n",
        "    return moves\n",
        "\n",
        "def get_gold_moves(n0, n, stack, heads, gold):\n",
        "    def deps_between(target, others, gold):\n",
        "        for word in others:\n",
        "            if gold[word] == target or gold[target] == word:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    valid = get_valid_moves(n0, n, len(stack))\n",
        "    if not stack or (SHIFT in valid and gold[n0] == stack[-1]):\n",
        "        return [SHIFT]\n",
        "    if gold[stack[-1]] == n0:\n",
        "        return [LEFT]\n",
        "    costly = set([m for m in MOVES if m not in valid])\n",
        "    if len(stack) >= 2 and gold[stack[-1]] == stack[-2]:\n",
        "        costly.add(LEFT)\n",
        "    if SHIFT not in costly and deps_between(n0, stack, gold):\n",
        "        costly.add(SHIFT)\n",
        "    if deps_between(stack[-1], range(n0 + 1, n - 1), gold):\n",
        "        costly.add(LEFT)\n",
        "        costly.add(RIGHT)\n",
        "    return [m for m in MOVES if m not in costly]\n",
        "\n",
        "def extract_features(words, tags, n0, n, stack, parse):\n",
        "    def get_stack_context(depth, stack, data):\n",
        "        if depth >= 3:\n",
        "            return data[stack[-1]], data[stack[-2]], data[stack[-3]]\n",
        "        elif depth >= 2:\n",
        "            return data[stack[-1]], data[stack[-2]], ''\n",
        "        elif depth == 1:\n",
        "            return data[stack[-1]], '', ''\n",
        "        else:\n",
        "            return '', '', ''\n",
        "\n",
        "    def get_buffer_context(i, n, data):\n",
        "        if i + 1 >= n:\n",
        "            return data[i], '', ''\n",
        "        elif i + 2 >= n:\n",
        "            return data[i], data[i + 1], ''\n",
        "        else:\n",
        "            return data[i], data[i + 1], data[i + 2]\n",
        "\n",
        "    def get_parse_context(word, deps, data):\n",
        "        if word == -1:\n",
        "            return 0, '', ''\n",
        "        deps = deps[word]\n",
        "        valency = len(deps)\n",
        "        if not valency:\n",
        "            return 0, '', ''\n",
        "        elif valency == 1:\n",
        "            return 1, data[deps[-1]], ''\n",
        "        else:\n",
        "            return valency, data[deps[-1]], data[deps[-2]]\n",
        "\n",
        "    features = {}\n",
        "    depth = len(stack)\n",
        "    s0 = stack[-1] if depth else -1\n",
        "\n",
        "    Ws0, Ws1, Ws2 = get_stack_context(depth, stack, words)\n",
        "    Ts0, Ts1, Ts2 = get_stack_context(depth, stack, tags)\n",
        "\n",
        "    Wn0, Wn1, Wn2 = get_buffer_context(n0, n, words)\n",
        "    Tn0, Tn1, Tn2 = get_buffer_context(n0, n, tags)\n",
        "\n",
        "    Vn0b, Wn0b1, Wn0b2 = get_parse_context(n0, parse.lefts, words)\n",
        "    Vn0b, Tn0b1, Tn0b2 = get_parse_context(n0, parse.lefts, tags)\n",
        "\n",
        "    Vn0f, Wn0f1, Wn0f2 = get_parse_context(n0, parse.rights, words)\n",
        "    _, Tn0f1, Tn0f2 = get_parse_context(n0, parse.rights, tags)\n",
        "\n",
        "    Vs0b, Ws0b1, Ws0b2 = get_parse_context(s0, parse.lefts, words)\n",
        "    _, Ts0b1, Ts0b2 = get_parse_context(s0, parse.lefts, tags)\n",
        "\n",
        "    Vs0f, Ws0f1, Ws0f2 = get_parse_context(s0, parse.rights, words)\n",
        "    _, Ts0f1, Ts0f2 = get_parse_context(s0, parse.rights, tags)\n",
        "\n",
        "    Ds0n0 = min((n0 - s0, 5)) if s0 != 0 else 0\n",
        "\n",
        "    features['bias'] = 1\n",
        "    for w in (Wn0, Wn1, Wn2, Ws0, Ws1, Ws2, Wn0b1, Wn0b2, Ws0b1, Ws0b2, Ws0f1, Ws0f2):\n",
        "        if w:\n",
        "            features['w=%s' % w] = 1\n",
        "    for t in (Tn0, Tn1, Tn2, Ts0, Ts1, Ts2, Tn0b1, Tn0b2, Ts0b1, Ts0b2, Ts0f1, Ts0f2):\n",
        "        if t:\n",
        "            features['t=%s' % t] = 1\n",
        "    for v in (Vn0b, Vn0f, Vs0b, Vs0f):\n",
        "        if v:\n",
        "            features['v=%s' % v] = 1\n",
        "    if Ds0n0:\n",
        "        features['d=%d' % Ds0n0] = 1\n",
        "    return features\n",
        "\n",
        "class Perceptron(object):\n",
        "    def __init__(self, moves):\n",
        "        self.moves = moves\n",
        "        self.weights = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    def score(self, features):\n",
        "        scores = defaultdict(int)\n",
        "        for feat, value in features.items():\n",
        "            for move in self.moves:\n",
        "                scores[move] += self.weights[move][feat] * value\n",
        "        return scores\n",
        "\n",
        "    def update(self, gold, guess, features):\n",
        "        if gold == guess:\n",
        "            return\n",
        "        for feat, value in features.items():\n",
        "            self.weights[gold][feat] += value\n",
        "            self.weights[guess][feat] -= value\n",
        "\n",
        "    def save(self, filename):\n",
        "        with open(filename, 'wb') as f:\n",
        "            # Convert defaultdict to dict for pickling\n",
        "            weights = {k: dict(v) for k, v in self.weights.items()}\n",
        "            pickle.dump(weights, f)\n",
        "\n",
        "    def load(self, filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            self.weights = defaultdict(lambda: defaultdict(int), pickle.load(f))\n",
        "\n",
        "class PerceptronTagger(object):\n",
        "    def __init__(self, load=True):\n",
        "        self.model = Perceptron(['NN', 'VB', 'JJ'])\n",
        "        if load:\n",
        "            self.model.load('/content/drive/MyDrive/tagger.pickle')\n",
        "\n",
        "    def tag(self, words):\n",
        "        return ['NN' for _ in words]  # Dummy tagging\n",
        "\n",
        "    def save(self):\n",
        "        self.model.save('/content/drive/MyDrive/tagger.pickle')\n",
        "\n",
        "def train_parser(parser, corpus):\n",
        "    for words, heads in corpus:\n",
        "        tags, gold_heads = parser.parse(words)\n",
        "        parser.train_one(1, words, tags, gold_heads)\n",
        "\n",
        "# Usage\n",
        "parser = Parser(load=False)  # Load True if you have an existing model\n",
        "corpus = [(['I', 'love', 'Python'], [1, 2, 1])]  # Example corpus\n",
        "train_parser(parser, corpus)\n",
        "parser.save()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX-9DEzR8wy8",
        "outputId": "f4b52ce7-fccb-4b66-fc21-276f0e0c845c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_honnibal_format(sentence):\n",
        "    converted_sentence = []\n",
        "    for token in sentence:\n",
        "        if len(token) < 8:\n",
        "            continue  # Skip invalid tokens\n",
        "        form = token[1]  # FORM\n",
        "        head = int(token[6])  # HEAD\n",
        "        deprel = token[7]  # DEPREL\n",
        "        pos = token[3]  # UPOS\n",
        "\n",
        "        # Convert HEAD from 0 (CoNLL-U root) to -1 (Honnibal's parser root)\n",
        "        if head == 0:\n",
        "            head = -1\n",
        "\n",
        "        converted_sentence.append([form, str(head), deprel, pos])\n",
        "    return converted_sentence\n",
        "\n",
        "# Example of how to use the conversion function\n",
        "print(\"\\nConverted Sentences for Honnibal's Parser:\")\n",
        "for sentence_id, sentence_text, sentence in non_projective_sentences:\n",
        "    honnibal_format_sentence = convert_to_honnibal_format(sentence)\n",
        "    print(f\"ID: {sentence_id}\")\n",
        "    for token in honnibal_format_sentence:\n",
        "        print(\"\\t\".join(token))\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9OWQbOGDSSk",
        "outputId": "3d3233c1-a37e-473f-8eba-5cbc3161fe16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Converted Sentences for Honnibal's Parser:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from collections import defaultdict\n",
        "#https://raw.githubusercontent.com/UniversalDependencies/UD_German-HDT/dev/de_hdt-ud-dev.conll\n",
        "\n",
        "\n",
        "# MHD Bashir Kabbani 3017375\n",
        "# Vialli Tem Ndumbe 2911472\n",
        "\n",
        "# Step 1: Download the treebank file\n",
        "url = 'https://github.com/UniversalDependencies/UD_German-HDT.git'\n",
        "response = requests.get(url)\n",
        "file_content = response.text\n",
        "\n",
        "# Step 2: Parse the file content\n",
        "def parse_conllu(file_content):\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    sentence_meta = {}\n",
        "    for line in file_content.split('\\n'):\n",
        "        if line.startswith('#'):\n",
        "            if line.startswith('# sent_id'):\n",
        "                sentence_meta['id'] = line.split('=')[-1].strip()\n",
        "            elif line.startswith('# text'):\n",
        "                sentence_meta['text'] = line.split('=')[-1].strip()\n",
        "        elif line.strip() == '':\n",
        "            if sentence:\n",
        "                sentences.append((sentence_meta, sentence))\n",
        "                sentence = []\n",
        "                sentence_meta = {}\n",
        "        else:\n",
        "            sentence.append(line.split('\\t'))\n",
        "    return sentences\n",
        "\n",
        "# Step 3: Identify non-projective trees\n",
        "def is_non_projective(sentence):\n",
        "    arcs = []\n",
        "    for token in sentence:\n",
        "        if len(token) < 8:\n",
        "            continue  # Skip tokens with fewer than 8 elements\n",
        "        try:\n",
        "            head = int(token[6])\n",
        "            dependent = int(token[0])\n",
        "        except ValueError:\n",
        "            continue  # Skip tokens where head or id is not an integer\n",
        "        if head == 0:  # Skip root\n",
        "            continue\n",
        "        if head > dependent:\n",
        "            head, dependent = dependent, head\n",
        "        arcs.append((head, dependent))\n",
        "\n",
        "    for i in range(len(arcs)):\n",
        "        for j in range(i + 1, len(arcs)):\n",
        "            (h1, d1) = arcs[i]\n",
        "            (h2, d2) = arcs[j]\n",
        "            if (h1 < h2 < d1 < d2) or (h2 < h1 < d2 < d1):\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# Step 4: Extract non-projective sentences and their identifiers\n",
        "def extract_non_projective_sentences(file_content):\n",
        "    sentences = parse_conllu(file_content)\n",
        "    non_projective_sentences = []\n",
        "    for sentence_meta, sentence in sentences:\n",
        "        if is_non_projective(sentence):\n",
        "            non_projective_sentences.append((sentence_meta['id'], sentence_meta['text'], sentence))\n",
        "    return non_projective_sentences\n",
        "\n",
        "# Step 5: Main execution\n",
        "non_projective_sentences = extract_non_projective_sentences(file_content)\n",
        "print(\"Non-projective sentences:\")\n",
        "for sentence_id, sentence_text, sentence in non_projective_sentences:\n",
        "    print(f\"ID: {sentence_id}\")\n",
        "    print(f\"Text: {sentence_text}\")\n",
        "    print()\n",
        "\n",
        "# Step 5: Count non-projective sentences\n",
        "def count_non_projective_sentences(non_projective_sentences):\n",
        "    return len(non_projective_sentences)\n",
        "\n",
        "# Print the count of non-projective sentences\n",
        "total_non_projective = count_non_projective_sentences(non_projective_sentences)\n",
        "print(f\"Total number of non-projective sentences: {total_non_projective}\")\n",
        "\n",
        "# Exercise 2: Reordered list of words to make the tree projective\n",
        "def create_dependency_graph(sentence):\n",
        "    graph = defaultdict(list)\n",
        "    words = {}\n",
        "    for token in sentence:\n",
        "        if len(token) < 8:\n",
        "            continue\n",
        "        try:\n",
        "            head = int(token[6])\n",
        "            dependent = int(token[0])\n",
        "            word = token[1]\n",
        "        except ValueError:\n",
        "            continue  # Skip tokens where head or id is not an integer\n",
        "        graph[head].append(dependent)\n",
        "        words[dependent] = word\n",
        "    return graph, words\n",
        "\n",
        "def inorder_traversal(node, graph, words, result):\n",
        "    if node not in graph:\n",
        "        return\n",
        "    children = sorted(graph[node])\n",
        "    for child in children:\n",
        "        inorder_traversal(child, graph, words, result)\n",
        "    if node in words:\n",
        "        result.append(words[node])\n",
        "\n",
        "def make_projective(sentence):\n",
        "    graph, words = create_dependency_graph(sentence)\n",
        "    result = []\n",
        "    inorder_traversal(0, graph, words, result)\n",
        "    return result\n",
        "\n",
        "print(\"\\nExercise 2: Reordered sentences to make the tree projective:\")\n",
        "for sentence_id, sentence_text, sentence in non_projective_sentences:\n",
        "    projective_order = make_projective(sentence)\n",
        "    print(f\"ID: {sentence_id}\")\n",
        "    print(f\"Original Text: {sentence_text}\")\n",
        "    print(\"Projective Order:\", \" \".join(projective_order))\n",
        "    print()\n",
        "\n",
        "# Exercise 3: Analysis of syntactic construction types\n",
        "def analyze_construction_types(non_projective_sentences, num_sentences=50):\n",
        "    construction_types = {\n",
        "        'wh-movement/fronting': [],\n",
        "        'extraposition': [],\n",
        "        'parentheticals/insertions': []\n",
        "    }\n",
        "\n",
        "    for sentence_id, sentence_text, sentence in non_projective_sentences[:num_sentences]:\n",
        "        for token in sentence:\n",
        "            if len(token) >= 8:\n",
        "                construction = token[7]\n",
        "                if construction.startswith('wh'):\n",
        "                    construction_types['wh-movement/fronting'].append((sentence_id, sentence_text))\n",
        "                elif construction.startswith('extr'):\n",
        "                    construction_types['extraposition'].append((sentence_id, sentence_text))\n",
        "                elif construction.startswith('parataxis'):\n",
        "                    construction_types['parentheticals/insertions'].append((sentence_id, sentence_text))\n",
        "\n",
        "    return construction_types\n",
        "\n",
        "print(\"\\nExercise 3: Analysis of Non-Projective Sentences:\")\n",
        "construction_types = analyze_construction_types(non_projective_sentences)\n",
        "\n",
        "print(\"\\n## Most frequent syntactic construction type: Wh-movement/Fronting\")\n",
        "print(\"Examples:\")\n",
        "for sent_id, sent_text in construction_types['wh-movement/fronting']:\n",
        "    print(f\"- ID: {sent_id}, Text: {sent_text}\")\n",
        "\n",
        "print(\"\\n### Other construction types:\")\n",
        "print(\"1. Extraposition\")\n",
        "print(\"Examples:\")\n",
        "for sent_id, sent_text in construction_types['extraposition']:\n",
        "    print(f\"- ID: {sent_id}, Text: {sent_text}\")\n",
        "\n",
        "print(\"\\n2. Parentheticals/Insertions\")\n",
        "print(\"Examples:\")\n",
        "for sent_id, sent_text in construction_types['parentheticals/insertions']:\n",
        "    print(f\"- ID: {sent_id}, Text: {sent_text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH-mEgJSDjyA",
        "outputId": "bf528ff5-0aa9-4e76-9fc3-89e6dd096874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-projective sentences:\n",
            "Total number of non-projective sentences: 0\n",
            "\n",
            "Exercise 2: Reordered sentences to make the tree projective:\n",
            "\n",
            "Exercise 3: Analysis of Non-Projective Sentences:\n",
            "\n",
            "## Most frequent syntactic construction type: Wh-movement/Fronting\n",
            "Examples:\n",
            "\n",
            "### Other construction types:\n",
            "1. Extraposition\n",
            "Examples:\n",
            "\n",
            "2. Parentheticals/Insertions\n",
            "Examples:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_honnibal_format(sentence):\n",
        "    converted_sentence = []\n",
        "    for token in sentence:\n",
        "        if len(token) < 8:\n",
        "            continue  # Skip invalid tokens\n",
        "        form = token[1]  # FORM\n",
        "        head = int(token[6])  # HEAD\n",
        "        deprel = token[7]  # DEPREL\n",
        "        pos = token[3]  # UPOS\n",
        "\n",
        "        # Convert HEAD from 0 (CoNLL-U root) to -1 (Honnibal's parser root)\n",
        "        if head == 0:\n",
        "            head = -1\n",
        "\n",
        "        converted_sentence.append([form, str(head), deprel, pos])\n",
        "    return converted_sentence\n",
        "\n",
        "# Example of how to use the conversion function\n",
        "print(\"\\nConverted Sentences for Honnibal's Parser:\")\n",
        "for sentence_id, sentence_text, sentence in non_projective_sentences:\n",
        "    honnibal_format_sentence = convert_to_honnibal_format(sentence)\n",
        "    print(f\"ID: {sentence_id}\")\n",
        "    for token in honnibal_format_sentence:\n",
        "        print(\"\\t\".join(token))\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TthNnRj2Dmpb",
        "outputId": "292fdd28-055a-486b-aed1-130c91416ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Converted Sentences for Honnibal's Parser:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from collections import defaultdict\n",
        "\n",
        "# Step 1: Download the CoNLL-U file\n",
        "url = 'https://github.com/UniversalDependencies/UD_German-HDT.git'\n",
        "response = requests.get(url)\n",
        "file_content = response.text\n",
        "\n",
        "# Step 2: Parse the file content\n",
        "def parse_conllu(file_content):\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    sentence_meta = {}\n",
        "    for line in file_content.split('\\n'):\n",
        "        if line.startswith('#'):\n",
        "            if line.startswith('# sent_id'):\n",
        "                sentence_meta['id'] = line.split('=')[-1].strip()\n",
        "            elif line.startswith('# text'):\n",
        "                sentence_meta['text'] = line.split('=')[-1].strip()\n",
        "        elif line.strip() == '':\n",
        "            if sentence:\n",
        "                sentences.append((sentence_meta, sentence))\n",
        "                sentence = []\n",
        "                sentence_meta = {}\n",
        "        else:\n",
        "            sentence.append(line.split('\\t'))\n",
        "    return sentences\n",
        "\n",
        "# Step 3: Convert sentences to Honnibal's format\n",
        "def convert_to_honnibal_format(sentence):\n",
        "    converted_sentence = []\n",
        "    for token in sentence:\n",
        "        if len(token) < 8:\n",
        "            continue  # Skip invalid tokens\n",
        "        form = token[1]  # FORM\n",
        "        head = int(token[6])  # HEAD\n",
        "        deprel = token[7]  # DEPREL\n",
        "        pos = token[3]  # UPOS\n",
        "\n",
        "        # Convert HEAD from 0 (CoNLL-U root) to -1 (Honnibal's parser root)\n",
        "        if head == 0:\n",
        "            head = -1\n",
        "\n",
        "        converted_sentence.append([form, str(head), deprel, pos])\n",
        "    return converted_sentence\n",
        "\n",
        "# Step 4: Extract and convert all sentences\n",
        "def extract_and_convert_sentences(file_content):\n",
        "    sentences = parse_conllu(file_content)\n",
        "    converted_sentences = []\n",
        "    for sentence_meta, sentence in sentences:\n",
        "        honnibal_format_sentence = convert_to_honnibal_format(sentence)\n",
        "        converted_sentences.append((sentence_meta['id'], honnibal_format_sentence))\n",
        "    return converted_sentences\n",
        "\n",
        "# Execute the extraction and conversion\n",
        "converted_sentences = extract_and_convert_sentences(file_content)\n",
        "\n",
        "# Step 5: Print the converted sentences\n",
        "print(\"Converted Sentences for Honnibal's Parser:\")\n",
        "for sentence_id, sentence in converted_sentences:\n",
        "    print(f\"ID: {sentence_id}\")\n",
        "    for token in sentence:\n",
        "        print(\"\\t\".join(token))\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "Cjl4hSSXEutj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from collections import defaultdict\n",
        "\n",
        "# Step 1: Download the CoNLL-U file\n",
        "url = 'https://github.com/UniversalDependencies/UD_German-HDT.git'\n",
        "response = requests.get(url)\n",
        "file_content = response.text\n",
        "\n",
        "# Step 2: Parse the file content\n",
        "def parse_conllu(file_content):\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    sentence_meta = {}\n",
        "    for line in file_content.split('\\n'):\n",
        "        if line.startswith('#'):\n",
        "            if line.startswith('# sent_id'):\n",
        "                sentence_meta['id'] = line.split('=')[-1].strip()\n",
        "            elif line.startswith('# text'):\n",
        "                sentence_meta['text'] = line.split('=')[-1].strip()\n",
        "        elif line.strip() == '':\n",
        "            if sentence:\n",
        "                # Check if 'id' exists before appending\n",
        "                if 'id' in sentence_meta:\n",
        "                    sentences.append((sentence_meta, sentence))\n",
        "                sentence = []\n",
        "                sentence_meta = {}\n",
        "        else:\n",
        "            sentence.append(line.split('\\t'))\n",
        "    return sentences\n",
        "\n",
        "# Step 3: Convert sentences to Honnibal's format\n",
        "def convert_to_honnibal_format(sentence):\n",
        "    converted_sentence = []\n",
        "    for token in sentence:\n",
        "        if len(token) < 8:\n",
        "            continue  # Skip invalid tokens\n",
        "        form = token[1]  # FORM\n",
        "        head = int(token[6])  # HEAD\n",
        "        deprel = token[7]  # DEPREL\n",
        "        pos = token[3]  # UPOS\n",
        "\n",
        "        # Convert HEAD from 0 (CoNLL-U root) to -1 (Honnibal's parser root)\n",
        "        if head == 0:\n",
        "            head = -1\n",
        "\n",
        "        converted_sentences.append([form, str(head), deprel, pos])\n",
        "    return converted_sentence\n",
        "\n",
        "# Step 4: Extract and convert all sentences\n",
        "def extract_and_convert_sentences(file_content):\n",
        "    sentences = parse_conllu(file_content)\n",
        "    converted_sentences = []\n",
        "    for sentence_meta, sentence in sentences:\n",
        "        honnibal_format_sentence = convert_to_honnibal_format(sentence)\n",
        "        # Check if 'id' exists before appending\n",
        "        if 'id' in sentence_meta:\n",
        "            converted_sentences.append((sentence_meta['id'], honnibal_format_sentence))\n",
        "    return converted_sentences\n",
        "\n",
        "# Execute the extraction and conversion\n",
        "converted_sentences = extract_and_convert_sentences(file_content)\n",
        "\n",
        "# Step 5: Print the converted sentences\n",
        "print(\"Converted Sentences for Honnibal's Parser:\")\n",
        "for sentence_id, sentence in converted_sentences:\n",
        "    print(f\"ID: {sentence_id}\")\n",
        "    for token in sentence:\n",
        "        print(\"\\t\".join(token))\n",
        "    print()"
      ],
      "metadata": {
        "id": "Xbt9nzkRFb6x",
        "outputId": "da6aafe4-4ab0-4950-c384-968d62410597",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted Sentences for Honnibal's Parser:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/UniversalDependencies/UD_German-HDT.git\n",
        "%cd UD_German-HDT\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3KaBs3OzHP5",
        "outputId": "f43d9044-5510-41ee-fb3c-0c39591724e2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'UD_German-HDT'...\n",
            "remote: Enumerating objects: 1324, done.\u001b[K\n",
            "remote: Counting objects: 100% (164/164), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 1324 (delta 124), reused 136 (delta 98), pack-reused 1160 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1324/1324), 2.47 GiB | 21.72 MiB/s, done.\n",
            "Resolving deltas: 100% (1088/1088), done.\n",
            "Updating files: 100% (11/11), done.\n",
            "/content/UD_German-HDT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "UfDji73V3Ael",
        "outputId": "5c6b5f21-d713-42f7-dca8-59978ba84170"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d3c3d772-cfeb-4785-baa6-9078aba74cc4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d3c3d772-cfeb-4785-baa6-9078aba74cc4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving GISTFI~1.PY to GISTFI~1.PY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory to where the files were cloned\n",
        "%cd UD_German-HDT\n",
        "\n",
        "# List the files in the directory\n",
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABcaodxz3lEm",
        "outputId": "ddfd9ee4-2f2f-4737-ca60-913931ec2d2d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'UD_German-HDT'\n",
            "/content/UD_German-HDT\n",
            "CONTRIBUTING.md        de_hdt-ud-train-a-1.conllu  de_hdt-ud-train-b-2.conllu  LICENSE.txt\n",
            "de_hdt-ud-dev.conllu   de_hdt-ud-train-a-2.conllu  eval.log\t\t       README.md\n",
            "de_hdt-ud-test.conllu  de_hdt-ud-train-b-1.conllu  gistfile1.PY\t\t       stats.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install conllu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKYO-XcV3x59",
        "outputId": "663afbdd-29e2-4ed1-a3c0-5f55f4ed89db"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting conllu\n",
            "  Downloading conllu-5.0.1-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading conllu-5.0.1-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-5.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from conllu import parse_incr\n",
        "\n",
        "# Function to check projectivity\n",
        "def is_projective(tree):\n",
        "    n = len(tree)\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            head1, dep1 = tree[i]\n",
        "            head2, dep2 = tree[j]\n",
        "            if head1 > dep1:\n",
        "                head1, dep1 = dep1, head1\n",
        "            if head2 > dep2:\n",
        "                head2, dep2 = dep2, head2\n",
        "            if (head1 < head2 < dep1 < dep2) or (head2 < head1 < dep2 < dep1):\n",
        "                return False\n",
        "    return True\n",
        "\n",
        "# Function to filter non-projective trees\n",
        "def filter_non_projective_trees(conllu_file, output_file):\n",
        "    with open(conllu_file, 'r') as f:\n",
        "        trees = list(parse_incr(f))\n",
        "\n",
        "    projective_trees = []\n",
        "\n",
        "    for tree in trees:\n",
        "        # Handle None values for 'head'\n",
        "        edges = [(int(token['head']), int(token['id'])) for token in tree if token['head'] is not None and token['head'] != 0]\n",
        "        if is_projective(edges):\n",
        "            projective_trees.append(tree)\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        for tree in projective_trees:\n",
        "            f.write(tree.serialize())\n",
        "\n",
        "# Function to convert to parser format\n",
        "def convert_conllu_to_parser_format(conllu_file, output_file):\n",
        "    with open(conllu_file, 'r') as f:\n",
        "        trees = list(parse_incr(f))\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        for tree in trees:\n",
        "            for token in tree:\n",
        "                # Handle None values for 'head'\n",
        "                head = token['head'] - 1 if token['head'] is not None and token['head'] > 0 else -1\n",
        "                f.write(f\"{token['id']}\\t{token['form']}\\t{head}\\t{token['deprel']}\\n\")\n",
        "            f.write('\\n')  # Separate sentences with a blank line\n",
        "\n",
        "# Example usage:\n",
        "# Replace 'de_hdt-ud-train.conllu' with the actual path if it's in a subfolder\n",
        "filter_non_projective_trees('/content/UD_German-HDT/de_hdt-ud-train-a-1.conllu', 'de_hdt-ud-train-projective.conllu')\n",
        "convert_conllu_to_parser_format('de_hdt-ud-train-projective.conllu', 'parser_input_format.txt')"
      ],
      "metadata": {
        "id": "ckEC-tZW89tl"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h4_A6Ka9LBv",
        "outputId": "23dc9191-1e8e-44f3-cb75-e57e4f6efc73"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONTRIBUTING.md\t\t    de_hdt-ud-train-a-2.conllu\t       eval.log\t\t\tREADME.md\n",
            "de_hdt-ud-dev.conllu\t    de_hdt-ud-train-b-1.conllu\t       gistfile1.PY\t\tstats.xml\n",
            "de_hdt-ud-test.conllu\t    de_hdt-ud-train-b-2.conllu\t       LICENSE.txt\n",
            "de_hdt-ud-train-a-1.conllu  de_hdt-ud-train-projective.conllu  parser_input_format.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd de_hdt-ud-train-projective.conllu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WujREOk49cQG",
        "outputId": "500226a3-fa45-48a9-e406-e10b817e823c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 20] Not a directory: 'de_hdt-ud-train-projective.conllu'\n",
            "/content/UD_German-HDT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.view('de_hdt-ud-train-projective.conllu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "MJf48orK9thU",
        "outputId": "61714670-027f-48d8-9878-cb499d62ae6a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/content/UD_German-HDT/de_hdt-ud-train-projective.conllu\")"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.view('parser_input_format.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "nk7kyjD9-a9L",
        "outputId": "ebd9fb76-104b-4946-bda6-9971ddf1e95f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/content/UD_German-HDT/parser_input_format.txt\")"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to view the content of a file\n",
        "def view_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        content = file.read()\n",
        "        print(content[:2000])  # Print the first 2000 characters for brevity\n",
        "\n",
        "# View the projective CoNLL-U file\n",
        "view_file('de_hdt-ud-train-projective.conllu')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "A9U0oD8T-axj",
        "outputId": "a6159c07-3c85-4bc6-e61b-ba53e819b325"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# sent_id = hdt-s20001\n",
            "# text = Hinter der neuen Firma steht unter anderem Lucent Technologies , einer der grten Anbieter von Equipment fr Netzwerke und Telekommunikation .\n",
            "1\tHinter\tHinter\tADP\tAPPR\tAdpType=Prep|Case=Dat\t4\tcase\t_\t_\n",
            "2\tder\tder\tDET\tART\tCase=Dat|Definite=Def|Gender=Fem|Number=Sing|PronType=Art\t4\tdet\t_\t_\n",
            "3\tneuen\tneu\tADJ\tADJA\tDegree=Pos|Gender=Fem|Number=Sing\t4\tamod\t_\t_\n",
            "4\tFirma\tFirma\tNOUN\tNN\tGender=Fem|Number=Sing\t5\tobl\t_\t_\n",
            "5\tsteht\tstehen\tVERB\tVVFIN\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t0\troot\t_\t_\n",
            "6\tunter\tunter\tADP\tAPPR\tAdpType=Prep|Case=Dat\t7\tcase\t_\t_\n",
            "7\tanderem\tanderer\tDET\tPIS\tCase=Dat|Gender=Neut|Number=Sing|PronType=Ind\t8\tdet\t_\t_\n",
            "8\tLucent\tLucent\tPROPN\tNE\tCase=Nom|Number=Sing\t5\tnsubj\t_\t_\n",
            "9\tTechnologies\tTechnologies\tX\tFM\tForeign=Yes\t8\tflat\t_\t_\n",
            "10\t,\t,\tPUNCT\t$,\tPunctType=Comm\t11\tpunct\t_\t_\n",
            "11\teiner\tein\tDET\tPIS\tCase=Nom|Definite=Ind|Gender=Masc|Number=Sing|NumType=Card|PronType=Art\t8\tappos\t_\t_\n",
            "12\tder\tder\tDET\tART\tCase=Gen|Definite=Def|Number=Plur|PronType=Art\t14\tdet\t_\t_\n",
            "13\tgrten\tgro\tADJ\tADJA\tCase=Gen|Degree=Sup|Gender=Masc|Number=Plur\t14\tamod\t_\t_\n",
            "14\tAnbieter\tAnbieter\tNOUN\tNN\tGender=Masc|Number=Plur\t11\tnmod\t_\t_\n",
            "15\tvon\tvon\tADP\tAPPR\tAdpType=Prep|Case=Dat\t16\tcase\t_\t_\n",
            "16\tEquipment\tEquipment\tNOUN\tNN\tGender=Neut|Number=Sing\t14\tnmod\t_\t_\n",
            "17\tfr\tfr\tADP\tAPPR\tAdpType=Prep\t18\tcase\t_\t_\n",
            "18\tNetzwerke\tNetzwerk\tNOUN\tNN\tGender=Neut|Number=Plur\t16\tnmod\t_\t_\n",
            "19\tund\tund\tCCONJ\tKON\t_\t20\tcc\t_\t_\n",
            "20\tTelekommunikation\tTelekommunikation\tNOUN\tNN\tGender=Fem|Number=Sing\t18\tconj\t_\t_\n",
            "21\t.\t.\tPUNCT\t$.\tPunctType=Peri\t5\tpunct\t_\t_\n",
            "\n",
            "# sent_id = hdt-s20002\n",
            "# text = Wirtschaftsministerium :\n",
            "1\tWirtschaftsministerium\tMinisterium\tNOUN\tNN\tGender=Neut|Number=Sing\t0\troot\t_\t_\n",
            "2\t:\t:\tPUNCT\t$.\tPunctType=Peri\t1\tpunct\t_\t_\n",
            "\n",
            "# sent_id = hdt-s20003\n",
            "# text = 20 Millionen DSL-Anschlsse bis 2010\n",
            "1\t20\t20\tNUM\tCARD\tNumber=Plur|NumType=Card\t2\tnummod\t_\t_\n",
            "2\tMillionen\tMillion\tNOUN\tNN\tGender=Fem|Number=Plur\t3\tnummod\t_\t_\n",
            "3\tDSL-Anschlsse\tAnschlu\tNOUN\tNN\tGender=Masc|Number=Plur\t0\troot\t_\t_\n",
            "4\tbis\tbis\tADP\tAP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View the parser format file\n",
        "view_file('parser_input_format.txt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HW8rStr_-aqH",
        "outputId": "b92195f4-56ee-4515-b312-385da3895766"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\tHinter\t3\tcase\n",
            "2\tder\t3\tdet\n",
            "3\tneuen\t3\tamod\n",
            "4\tFirma\t4\tobl\n",
            "5\tsteht\t-1\troot\n",
            "6\tunter\t6\tcase\n",
            "7\tanderem\t7\tdet\n",
            "8\tLucent\t4\tnsubj\n",
            "9\tTechnologies\t7\tflat\n",
            "10\t,\t10\tpunct\n",
            "11\teiner\t7\tappos\n",
            "12\tder\t13\tdet\n",
            "13\tgrten\t13\tamod\n",
            "14\tAnbieter\t10\tnmod\n",
            "15\tvon\t15\tcase\n",
            "16\tEquipment\t13\tnmod\n",
            "17\tfr\t17\tcase\n",
            "18\tNetzwerke\t15\tnmod\n",
            "19\tund\t19\tcc\n",
            "20\tTelekommunikation\t17\tconj\n",
            "21\t.\t4\tpunct\n",
            "\n",
            "1\tWirtschaftsministerium\t-1\troot\n",
            "2\t:\t0\tpunct\n",
            "\n",
            "1\t20\t1\tnummod\n",
            "2\tMillionen\t2\tnummod\n",
            "3\tDSL-Anschlsse\t-1\troot\n",
            "4\tbis\t4\tcase\n",
            "5\t2010\t2\tnmod\n",
            "\n",
            "(1, '-', 2)\tIm\t-1\t_\n",
            "1\tIn\t2\tcase\n",
            "2\tdem\t2\tdet\n",
            "3\tAuftrag\t5\tobl\n",
            "4\tdes\t4\tdet\n",
            "5\tBundeswirtschaftsministeriums\t2\tnmod\n",
            "6\terstellte\t-1\troot\n",
            "7\tdas\t8\tdet\n",
            "8\tWissenschaftliche\t8\tamod\n",
            "9\tInstitut\t5\tnsubj\n",
            "10\tfr\t10\tcase\n",
            "11\tKommunikationsdienste\t8\tnmod\n",
            "12\t(\t12\tpunct\n",
            "13\tWIK\t8\tappos\n",
            "14\t)\t12\tpunct\n",
            "15\tdie\t15\tdet\n",
            "16\tStudie\t5\tobj\n",
            "17\t\"\t17\tpunct\n",
            "18\tEntwicklungstrends\t15\tappos\n",
            "(19, '-', 20)\tim\t-1\t_\n",
            "19\tin\t20\tcase\n",
            "20\tdem\t20\tdet\n",
            "21\tTelekommunikationssektor\t17\tnmod\n",
            "22\tbis\t22\tcase\n",
            "23\t2010\t17\tnmod\n",
            "24\t\"\t17\tpunct\n",
            "25\t.\t5\tpunct\n",
            "\n",
            "1\tHierin\t1\tadvmod\n",
            "2\tprognostiziert\t-1\troot\n",
            "3\tdas\t3\tdet\n",
            "4\tInstitut\t1\tnsubj\n",
            "5\t,\t15\tpunct\n",
            "6\tdass\t15\tmark\n",
            "7\tes\t15\tnsubj\n",
            "(8, '-', 9)\tim\t-1\t_\n",
            "8\tin\t9\tcase\n",
            "9\tdem\t9\tdet\n",
            "10\tJahr\t15\tobl\n",
            "11\t2010\t9\tnmod\n",
            "12\trund\t12\tadvmod\n",
            "13\t20\t13\tnummod\n",
            "14\tMillionen\t14\tnummod\n",
            "15\tDSL-Anschlsse\t15\tobj\n",
            "16\tgeben\t1\tccomp\n",
            "17\twird\t15\taux\n",
            "18\t.\t1\tpunct\n",
            "\n",
            "1\tDie\t2\tdet\n",
            "2\tgrte\t2\tamod\n",
            "3\tKonkurrenz\t11\tnsubj\n",
            "4\tzu\t5\tcase\n",
            "5\tden\t5\tdet\n",
            "6\tDSL-Anbietern\t2\tnmod\n",
            "7\tdrfte\t11\taux\n",
            "8\tdemnach\t11\tadvmod\n",
            "9\tvon\t10\tcase\n",
            "10\tden\t10\tdet\n",
            "11\tKabelnetzbetreibern\t11\tobj\n",
            "12\tausgehen\t-1\troot\n",
            "13\t,\t20\tpunct\n",
            "14\t18\t14\tnummod\n",
            "15\tMillionen\t15\tnummod\n",
            "16\tBreitbandkabelanschlsse\t20\tobj\n",
            "17\twird\t20\taux\n",
            "18\tes\t20\tnsubj\n",
            "19\tlaut\t19\tcase\n",
            "20\tStudie\t20\tobl\n",
            "21\tgeben\t11\tconj\n",
            "22\t.\t11\tpunct\n",
            "\n",
            "1\tDie\t1\tdet\n",
            "2\tMarktchancen\t4\tobj\n",
            "3\tfr\t3\tcase\n",
            "4\tPowerline\t1\tnmod\n",
            "5\tschtzt\t-1\troot\n",
            "6\tdie\t6\tdet\n",
            "7\tStudie\t4\tnsubj\n",
            "8\thingegen\t4\tadvmod\n",
            "9\tgeringer\t4\tadvmod\n",
            "10\tein\t4\tcompound:prt\n",
            "11\t:\t4\tpunct\n",
            "12\tfnf\t12\tnummod\n",
            "13\tMillionen\t13\tnummod\n",
            "14\tHaushalte\t19\tnsubj\n",
            "15\tsollen\t19\taux\n",
            "16\t2010\t19\tobl\n",
            "17\tber\t18\tcase\n",
            "18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the projective CoNLL-U file\n",
        "files.download('de_hdt-ud-train-projective.conllu')\n",
        "\n",
        "# Download the parser format file\n",
        "files.download('parser_input_format.txt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Wsd_vPyk_V9D",
        "outputId": "5a3effe9-2e9d-4539-c956-12870acb21f1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_639d22c3-986e-4fc7-a325-69dd6812c6cc\", \"de_hdt-ud-train-projective.conllu\", 45878947)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_825b8140-c000-4cf2-b9bc-1c107a76e2be\", \"parser_input_format.txt\", 11033286)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i06zLNbmAPMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fi8xdg2VAPBB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}